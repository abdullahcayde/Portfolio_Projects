{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5528f92-f973-4a97-bee5-895e01f101de",
   "metadata": {},
   "source": [
    "# \"Web Scraping with Selenium to Find a Job\" \n",
    "\n",
    "We will go through 3 main tasks to implement our project:\n",
    "\n",
    "Task 1: Importing libraries.\n",
    "\n",
    "Task 2: Define functions.\n",
    "\n",
    "Task 3: Web scraping with selenium."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc42ec9-e367-454d-a1e7-290cfd658ce9",
   "metadata": {},
   "source": [
    "# Task 1 : Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7746b00f-5b5a-4866-831d-12b56c715dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfbfb6e-6233-4abe-9305-9b421869d22d",
   "metadata": {},
   "source": [
    "# Task 2 : Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f68a27f-889d-4c30-830e-1e1c54a38a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sleep function \n",
    "def sleep(x):\n",
    "    time.sleep(x)\n",
    "\n",
    "# Wait for a certain measure of time before throwing an exception\n",
    "def wait(x):\n",
    "    driver.implicitly_wait(x)\n",
    "\n",
    "# Click Function\n",
    "def click_bann_byID(ID):\n",
    "    actions = ActionChains(driver)\n",
    "    akzeptieren = driver.find_element(By.ID, ID)\n",
    "    actions.click(akzeptieren).perform()\n",
    "    wait(10)\n",
    "    sleep(0.5)\n",
    "\n",
    "# Find Elements Function\n",
    "def find_elements_HPCO(H,P,C,O):\n",
    "    if website_name == 'jobware':\n",
    "        header = driver.find_elements(By.TAG_NAME, H)\n",
    "    else:\n",
    "        header = driver.find_elements(By.CLASS_NAME, H)\n",
    "    publish = driver.find_elements(By.CLASS_NAME, P)\n",
    "    company = driver.find_elements(By.CLASS_NAME, C)\n",
    "    ort = driver.find_elements(By.CLASS_NAME, O) \n",
    "\n",
    "    list_header = [title.text for title in header]\n",
    "    list_publish = [pub.text for pub in publish]\n",
    "    list_company = [comp.text for comp in company]\n",
    "    list_ort = [o.text for o in ort]\n",
    "    return list_header, list_publish, list_company, list_ort\n",
    "\n",
    "# Scroll Down Function\n",
    "def scroll_down(x):\n",
    "    n=0\n",
    "    while n < x:\n",
    "        n+=1\n",
    "        actions.key_down(Keys.PAGE_DOWN).perform()\n",
    "        sleep(1.5)\n",
    "        actions.key_down(Keys.PAGE_DOWN).perform()\n",
    "        sleep(1.5)\n",
    "        actions.key_down(Keys.PAGE_DOWN).perform()\n",
    "        sleep(1.5)\n",
    "        actions.key_down(Keys.PAGE_UP).perform()\n",
    "        sleep(0.10)\n",
    "        actions.key_down(Keys.PAGE_DOWN).perform()\n",
    "        wait(10)\n",
    "        sleep(2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3feb03-1eeb-40e8-aa2a-48d4fea21a1a",
   "metadata": {},
   "source": [
    "# Web Scraping with Selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f24bb0e-db35-4c09-950f-541b8871c539",
   "metadata": {},
   "source": [
    "## 01 - STEPSTONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "962f38df-d454-44d7-afcc-8b1e7917ae67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------- StepStone Job Searching Selenium Project ----------------------\n",
      "Header 25 Publish 25 Company 25 Ort 25 Desc 25 Link 25\n",
      "Number of Jobs Pages = 21\n",
      "Page Number : 2, DataFrame Shape : (25, 6)\n",
      "Page Number : 3, DataFrame Shape : (25, 6)\n",
      "Page Number : 4, DataFrame Shape : (25, 6)\n",
      "Page Number : 5, DataFrame Shape : (25, 6)\n",
      "Page Number : 6, DataFrame Shape : (25, 6)\n",
      "Page Number : 7, DataFrame Shape : (25, 6)\n",
      "Page Number : 8, DataFrame Shape : (25, 6)\n",
      "Page Number : 9, DataFrame Shape : (25, 6)\n",
      "Page Number : 10, DataFrame Shape : (25, 6)\n",
      "Page Number : 11, DataFrame Shape : (25, 6)\n",
      "Page Number : 12, DataFrame Shape : (25, 6)\n",
      "Page Number : 13, DataFrame Shape : (25, 6)\n",
      "Page Number : 14, DataFrame Shape : (25, 6)\n",
      "Page Number : 15, DataFrame Shape : (25, 6)\n",
      "Page Number : 16, DataFrame Shape : (25, 6)\n",
      "Page Number : 17, DataFrame Shape : (25, 6)\n",
      "Page Number : 18, DataFrame Shape : (25, 6)\n",
      "Page Number : 19, DataFrame Shape : (25, 6)\n",
      "Page Number : 20, DataFrame Shape : (25, 6)\n",
      "Page Number : 21, DataFrame Shape : (19, 6)\n",
      "DataFrame End : (519, 6)\n",
      "Code Runned No Problem\n",
      "Time = 0:04:37.095376\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Title : Web Scrapping by Selenium \n",
    "Project Purpose: From StepStone scrap data for some Job Titels\n",
    "1 - Create Driver\n",
    "2 - Go to Website\n",
    "3 - Create ActionChain Object\n",
    "    3.1 - Click Banned \n",
    "4 - Take Title and Infos from Page\n",
    "    4.1 - Create Lists \n",
    "    4.2 - Create DataFrame\n",
    "    4.3 - Repeat Process\n",
    "    4.4 - Print and Save DataFrame\n",
    "'''\n",
    "\n",
    "print('---------------------- StepStone Job Searching Selenium Project ----------------------')\n",
    "start=datetime.now()  \n",
    "# Link Descriptions\n",
    "link_original_stepstone = 'https://www.stepstone.de/jobs/data-analyst/in-rietberg?radius=50&page=2'\n",
    "\n",
    "website_name = 'stepstone'\n",
    "job_name = 'Data Analyst'\n",
    "#job_name = 'Business Analyst'\n",
    "ort_ = 'Rietberg'\n",
    "radius = 50\n",
    "page_number = 1\n",
    "\n",
    "#  1 - Create Driver\n",
    "Path = '/Users/macbook/Desktop/projects/Github_Repositories/Portfolio Projects/02 - Web_Scraping_Job_Search/chromedriver'\n",
    "driver = webdriver.Chrome(Path)\n",
    "\n",
    "#  2 - Go to Website\n",
    "job_link = job_name.replace(' ', '-').lower()\n",
    "ort_link = ort_.lower()\n",
    "link = f'https://www.stepstone.de/jobs/{job_link}/in-{ort_link}?radius={radius}&page={page_number}'\n",
    "\n",
    "driver.get(link)\n",
    "wait(10)\n",
    "sleep(2)\n",
    "\n",
    "#  3 - ActionChain Object created\n",
    "# 3.1 - Click Banned Accept\n",
    "ID = 'ccmgt_explicit_accept'\n",
    "click_bann_byID(ID)\n",
    "\n",
    "# 4 -  Take Infos from Page\n",
    "# 4.1 - Headers, Publish_Time ,Company, City\n",
    "H, P, C, O = 'resultlist-12iu5pk', 'resultlist-3asi6i', 'resultlist-1v262t5', 'resultlist-dettfq'\n",
    "list_header, list_publish, list_company, list_ort = find_elements_HPCO(H,P,C,O)\n",
    "\n",
    "# 4.2 - Description and Page number of results\n",
    "description = driver.find_elements(By.CLASS_NAME, 'resultlist-1pq4x2u')\n",
    "result = driver.find_elements(By.CLASS_NAME, 'resultlist-xeyevn')\n",
    "\n",
    "\n",
    "# 4.3 - Get Links\n",
    "header = driver.find_elements(By.CLASS_NAME, H)\n",
    "list_link = [link.get_attribute('href') for link in header]\n",
    "\n",
    "# 4.4 - Get Texts for each finding\n",
    "list_description = [des.text for des in description]\n",
    "print('Header',len(list_header), 'Publish',len(list_publish), 'Company',len(list_company), 'Ort',len(list_ort), 'Desc', len(list_description), 'Link',len(list_link))\n",
    "\n",
    "# 4.5 - Total Search Page Number\n",
    "list_result = [res.text for res in result]\n",
    "number_of_page = int(list_result[0].split(' ')[-1])\n",
    "print(f'Number of Jobs Pages = {number_of_page}')\n",
    "\n",
    "# 4.6 - DataFrame df\n",
    "d = dict(job_title=np.array(list_header), publish=np.array(list_publish), company=np.array(list_company), city=np.array(list_ort) , description=np.array(list_description), link=np.array(list_link))\n",
    "df = pd.DataFrame.from_dict(d, orient='index')\n",
    "df = df.T\n",
    "\n",
    "\n",
    "# 4.7 Repeat Process for every Web Page\n",
    "while  page_number < number_of_page:\n",
    "    page_number+=1\n",
    "    \n",
    "    # 4.7.1 - Go to another page\n",
    "    link = f'https://www.stepstone.de/jobs/{job_link}/in-{ort_link}?radius={radius}&page={page_number}'\n",
    "    driver.get(link)\n",
    "    wait(10)\n",
    "    sleep(1.5)\n",
    "    \n",
    "    # 4.7.2 - Find the elements and get the Texts\n",
    "    list_header, list_publish, list_company, list_ort = find_elements_HPCO(H,P,C,O) \n",
    "    description = driver.find_elements(By.CLASS_NAME, 'resultlist-1pq4x2u')\n",
    "    list_description = [des.text for des in description]\n",
    "    header = driver.find_elements(By.CLASS_NAME, H)\n",
    "    list_link = [link.get_attribute('href') for link in header]\n",
    " \n",
    "    # 4.7.3 - Create new page Dataframe\n",
    "    d = dict(job_title=np.array(list_header), publish=np.array(list_publish), company=np.array(list_company), city=np.array(list_ort) , description=np.array(list_description), link=np.array(list_link))\n",
    "    df2 = pd.DataFrame.from_dict(d, orient='index')\n",
    "    df2 = df2.T\n",
    "    \n",
    "    # 4.7.4 - Concatenate the DataFrames\n",
    "    df = pd.concat([df,df2], axis=0, ignore_index=True)\n",
    "    print(f'Page Number : {page_number}, DataFrame Shape : {df2.shape}')\n",
    "    \n",
    "\n",
    "# 5.1 - Save Data as csv \n",
    "print(f'DataFrame End : {df.shape}')\n",
    "df['website'] = website_name\n",
    "\n",
    "path = '/Users/macbook/Desktop/projects/Github_Repositories/Portfolio Projects/02 - Web_Scraping_Job_Search/data'\n",
    "job_name2 = job_name.replace(' ', '-')\n",
    "time_ = datetime.today().strftime('%Y-%m-%d')\n",
    "df.to_csv(f'{path}/{job_name2}-{time_}.csv', index=False)\n",
    "\n",
    "# 6 - Quit\n",
    "end =datetime.now() \n",
    "print('Code Runned No Problem')\n",
    "print(f'Time = {end - start}')\n",
    "sleep(5)\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bba6fe-9b6e-4110-8eb6-2c9444a77965",
   "metadata": {},
   "source": [
    "## 02 - JOBWARE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74123825-811b-4b9e-b02f-1d35ac17737b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------- Jobware Job Searching Selenium Project ----------------------\n",
      "Header 15 Publish 15 Company 15 Ort 15 Desc 14 Link 15\n",
      "['15 Treffer\\nSortierung: Relevanz - Datum']\n",
      "DataFrame End : (15, 6)\n",
      "Code Runned No Problem\n",
      "Time = 0:00:12.665651\n"
     ]
    }
   ],
   "source": [
    "print('---------------------- Jobware Job Searching Selenium Project ----------------------')\n",
    "\n",
    "start=datetime.now()  \n",
    "# 0 Link Descriptions\n",
    "link_original = 'https://www.jobware.de/jobsuche?jw_jobname=data%20analyst&jw_jobort=333**%20Rietberg&jw_ort_distance=50'\n",
    "\n",
    "website_name = 'jobware'\n",
    "radius = 50\n",
    "page_number = 0\n",
    "\n",
    "#  1 - Create Driver\n",
    "Path = '/Users/macbook/Desktop/projects/Github_Repositories/Portfolio Projects/02 - Web_Scraping_Job_Search/chromedriver'\n",
    "driver = webdriver.Chrome(Path)\n",
    "\n",
    "#  2 - Go to Website\n",
    "job_link = job_name.replace(' ', '%20').lower()\n",
    "ort_link = ort_.capitalize()\n",
    "link = f'https://www.jobware.de/jobsuche?jw_jobname={job_link}&jw_jobort=333**%20{ort_}&jw_ort_distance={radius}'\n",
    "\n",
    "driver.get(link)\n",
    "wait(10)\n",
    "sleep(2)\n",
    "\n",
    "#  3 - ActionChain Object created\n",
    "# 3.1 - Click Banned Accept\n",
    "actions = ActionChains(driver)\n",
    "akzeptieren = driver.find_element(By.XPATH, '/html/body/div[1]/div/div[3]/div[2]/button')\n",
    "actions.click(akzeptieren).perform()\n",
    "wait(10)\n",
    "sleep(0.5)\n",
    "\n",
    "\n",
    "# 4 -  Take Infos from Page\n",
    "# 4.1 - Headers, Company, City, Description\n",
    "H, P, C, O = 'h2', 'date', 'company', 'location'\n",
    "list_header, list_publish, list_company, list_ort = find_elements_HPCO(H,P,C,O)\n",
    "description = driver.find_elements(By.CLASS_NAME, 'task')\n",
    "list_description = [des.text for des in description]\n",
    "\n",
    "links = driver.find_elements(By.CLASS_NAME, 'job')\n",
    "list_link = [link.get_attribute('href') for link in links]\n",
    "\n",
    "print('Header',len(list_header), 'Publish',len(list_publish), 'Company',len(list_company), 'Ort',len(list_ort), 'Desc', len(list_description), 'Link',len(list_link))\n",
    "\n",
    "# 4.2 - Total Search Page Number\n",
    "result = driver.find_elements(By.CLASS_NAME, 'result-sort')\n",
    "list_result = [res.text for res in result]\n",
    "print(list_result)\n",
    "\n",
    "# 4.3 - DataFrame df\n",
    "d = dict(job_title=np.array(list_header), publish=np.array(list_publish), company=np.array(list_company), city=np.array(list_ort) , description=np.array(list_description), link=np.array(list_link))\n",
    "df = pd.DataFrame.from_dict(d, orient='index')\n",
    "df = df.T\n",
    "\n",
    "# 5.1 - Save Data as csv\n",
    "print(f'DataFrame End : {df.shape}')\n",
    "df['website'] = website_name\n",
    "\n",
    "df.to_csv(f'{path}/{job_name2}-{time_}.csv', mode='a', index=False, header=False)\n",
    "\n",
    "# 6.1 Quit\n",
    "end =datetime.now() \n",
    "print('Code Runned No Problem')\n",
    "print(f'Time = {end - start}')\n",
    "sleep(5)\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abde48a-5765-4f02-81fd-b32ce46555e1",
   "metadata": {},
   "source": [
    "## 03 - LINKEDIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dbf2fbe-1a18-43d0-b510-6c6015993c4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------- Linkedin Job Searching Selenium Project ----------------------\n",
      "Header 150 Publish 147 Company 135 Ort 150 Desc 14 Link 135\n",
      "Number of Jobs Pages = ['Rietberg, Kuzey Ren-Vestfalya, Almanya konumunda 869 Data Analyst iş ilanı (49 yeni)']\n",
      "DataFrame End : (150, 7)\n",
      "Code Runned No Problem\n",
      "Time = 0:00:47.908240\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>publish</th>\n",
       "      <th>company</th>\n",
       "      <th>city</th>\n",
       "      <th>description</th>\n",
       "      <th>link</th>\n",
       "      <th>website</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Analyst (w/m/d) Power BI mit Remote-Anteil</td>\n",
       "      <td>1 hafta önce</td>\n",
       "      <td>ATLAS TITAN Mitte GmbH</td>\n",
       "      <td>Gütersloh</td>\n",
       "      <td>None</td>\n",
       "      <td>https://de.linkedin.com/jobs/view/data-analyst...</td>\n",
       "      <td>linkedin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Analyst / Scientist (m/w/d)</td>\n",
       "      <td>3 hafta önce</td>\n",
       "      <td>Dr. Wolff Group</td>\n",
       "      <td>Bielefeld</td>\n",
       "      <td>None</td>\n",
       "      <td>https://de.linkedin.com/jobs/view/data-analyst...</td>\n",
       "      <td>linkedin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data / Business Analyst (m/w/d)</td>\n",
       "      <td>3 hafta önce</td>\n",
       "      <td>Dr. Wolff Group</td>\n",
       "      <td>Bielefeld</td>\n",
       "      <td>None</td>\n",
       "      <td>https://de.linkedin.com/jobs/view/data-busines...</td>\n",
       "      <td>linkedin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Analyst für Datenmodellierung und Kundenb...</td>\n",
       "      <td>1 ay önce</td>\n",
       "      <td>Lurse</td>\n",
       "      <td>Salzkotten</td>\n",
       "      <td>None</td>\n",
       "      <td>https://de.linkedin.com/jobs/view/data-analyst...</td>\n",
       "      <td>linkedin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Online Data Analyst (m,f,d)</td>\n",
       "      <td>3 gün önce</td>\n",
       "      <td>TELUS International</td>\n",
       "      <td>Bielefeld</td>\n",
       "      <td>None</td>\n",
       "      <td>https://de.linkedin.com/jobs/view/online-data-...</td>\n",
       "      <td>linkedin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           job_title       publish  \\\n",
       "0    Data Analyst (w/m/d) Power BI mit Remote-Anteil  1 hafta önce   \n",
       "1                   Data Analyst / Scientist (m/w/d)  3 hafta önce   \n",
       "2                    Data / Business Analyst (m/w/d)  3 hafta önce   \n",
       "3  Data Analyst für Datenmodellierung und Kundenb...     1 ay önce   \n",
       "4                        Online Data Analyst (m,f,d)    3 gün önce   \n",
       "\n",
       "                  company        city description  \\\n",
       "0  ATLAS TITAN Mitte GmbH   Gütersloh        None   \n",
       "1         Dr. Wolff Group   Bielefeld        None   \n",
       "2         Dr. Wolff Group   Bielefeld        None   \n",
       "3                   Lurse  Salzkotten        None   \n",
       "4     TELUS International   Bielefeld        None   \n",
       "\n",
       "                                                link   website  \n",
       "0  https://de.linkedin.com/jobs/view/data-analyst...  linkedin  \n",
       "1  https://de.linkedin.com/jobs/view/data-analyst...  linkedin  \n",
       "2  https://de.linkedin.com/jobs/view/data-busines...  linkedin  \n",
       "3  https://de.linkedin.com/jobs/view/data-analyst...  linkedin  \n",
       "4  https://de.linkedin.com/jobs/view/online-data-...  linkedin  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('---------------------- Linkedin Job Searching Selenium Project ----------------------')\n",
    "   \n",
    "\n",
    "start=datetime.now()  \n",
    "# 0 Link Descriptions\n",
    "link_original = 'https://www.linkedin.com/jobs/search/?currentJobId=3199974140&distance=25&keywords=data%20analyst&location=Rietberg' \n",
    "\n",
    "website_name =  'linkedin'\n",
    "radius = 40\n",
    "page_number = 1\n",
    "\n",
    "#  1 - Create Driver\n",
    "Path = '/Users/macbook/Desktop/projects/Github_Repositories/Portfolio Projects/02 - Web_Scraping_Job_Search/chromedriver'\n",
    "driver = webdriver.Chrome(Path)\n",
    "\n",
    "#  2 - Go to Website\n",
    "job_link = job_name.replace(' ', '%20').lower()\n",
    "\n",
    "link2 = f'https://www.linkedin.com/jobs/search/?distance=25&keywords={job_link}&location={ort_}'\n",
    "driver.get(link2)\n",
    "wait(10)\n",
    "sleep(2)\n",
    "\n",
    "#  3 - ActionChain Object created\n",
    "# 3.1 - Click Banned Accept\n",
    "actions = ActionChains(driver)\n",
    "akzeptieren = driver.find_element(By.TAG_NAME, 'button')\n",
    "actions.click(akzeptieren).perform()\n",
    "wait(10)\n",
    "sleep(0.5)\n",
    "\n",
    "# 3.2 - Scroll Down Function\n",
    "scroll_down(4)\n",
    "\n",
    "# 4 -  Take Infos from Page\n",
    "# 4.1 - Headers, Company, City, Description\n",
    "H, P, C, O = 'base-search-card__title', 'job-search-card__listdate', 'hidden-nested-link', 'job-search-card__location'\n",
    "list_header, list_publish, list_company, list_ort = find_elements_HPCO(H,P,C,O)\n",
    "\n",
    "# 4.2 - Link Lists\n",
    "links = driver.find_elements(By.CLASS_NAME, 'base-card__full-link')\n",
    "list_link = [link.get_attribute('href') for link in links]\n",
    "\n",
    "print('Header',len(list_header), 'Publish',len(list_publish), 'Company',len(list_company), 'Ort',len(list_ort), 'Desc', len(list_description), 'Link',len(list_link))\n",
    "\n",
    "# 4.3 - Total Search Page Number\n",
    "result = driver.find_elements(By.CLASS_NAME, 'results-context-header__context')\n",
    "list_result = [res.text for res in result]\n",
    "print(f'Number of Jobs Pages = {list_result}')\n",
    "\n",
    "\n",
    "# 4.4 - DataFrame df\n",
    "d = dict(job_title=np.array(list_header), publish=np.array(list_publish), company=np.array(list_company), city=np.array(list_ort) , description=np.array(list_description), link=np.array(list_link))\n",
    "df = pd.DataFrame.from_dict(d, orient='index')\n",
    "df = df.T\n",
    "df['description'] = None\n",
    "df['website'] = website_name\n",
    "\n",
    "# 5.1 - Save Data as csv \n",
    "print(f'DataFrame End : {df.shape}')\n",
    "df.loc[df.website =='linkedin', 'city'] = df.loc[df.website =='linkedin', 'city'].str.replace(', Kuzey Ren-Vestfalya, Almanya', '')\n",
    "df.to_csv(f'{path}/{job_name2}-{time_}.csv', mode='a', index=False, header=False)\n",
    "\n",
    "# 4.5 Quit\n",
    "end =datetime.now() \n",
    "print('Code Runned No Problem')\n",
    "print(f'Time = {end - start}')\n",
    "sleep(5)\n",
    "driver.quit()\n",
    "\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
